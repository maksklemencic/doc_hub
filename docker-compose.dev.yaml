services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ollama-init:
    image: ollama/ollama
    depends_on:
      - ollama
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=${OLLAMA_URL}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME}
    entrypoint: []
    env_file:
      - .env.dev
    command: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        sleep 3 &&
        echo 'Checking if model ${LLM_MODEL_NAME} exists...' &&
        if ! OLLAMA_HOST=${OLLAMA_URL} ollama list | grep -q '${LLM_MODEL_NAME}'; then
          echo 'Model ${LLM_MODEL_NAME} not found, pulling...' &&
          OLLAMA_HOST=${OLLAMA_URL} ollama pull ${LLM_MODEL_NAME}
        else
          echo 'Model ${LLM_MODEL_NAME} already exists, skipping pull.'
        fi &&
        echo 'Model setup complete!'
      "
    restart: "no"

  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: docker/backend/Dockerfile
    ports:
      - "8000:8000"
      - "5678:5678"
    env_file:
      - .env.dev
    environment:
      - PYTHONPATH=/app
    volumes:
      - ./backend:/app/backend
      - model_cache:/app/models
    depends_on:
      ollama-init:
        condition: service_completed_successfully

  # frontend:
  #   build:
  #     context: ./frontend
  #     dockerfile: ../docker/frontend/Dockerfile
  #   ports:
  #     - "3000:3000"
  #   env_file: .env
  #   depends_on:
  #     - backend

volumes:
  ollama_data:
  qdrant_data:
  model_cache: